# 开发环境和时间

我已经很久没有这么长时间的折腾过开发环境了，从开始打算基于DeepSeekOCR做图书识别到现在快一个月时间，我尝试了好几套方案。

方案一，在 Windows 下直接跑大模型，然后各种依赖库问题 vllm报错，系统底层库报错。查了半天，没有好方案，换方案。
方案二，在 Windows 下使用 Ollama 跑，可以跑，但是图片处理比较失望，无法直接从图片识别出版面信息，只能处理文本识别，放弃。
方案三，换 LM Stuio 继续，没有可用的模型。DeesSeekOCR模型还不支持这个平台，要自己转换模型，没尝试过，无法预估时间，放弃。
方案四，找开源库，Windows 下也无法正常运行，换。
方案五，使用PaddleOCR，又重新开始折腾环境。最终跑起来，依然不够理想，企图服务化模型服务，未遂。收获还是有的，了解了飞桨这套体系，本来以为PaddleOCR只是模型，实际飞桨是一整套AI体系，从执行套件，运行平台，模型开发，微调训练，推理环境全套。不过这也导致没有人把飞桨的模型转出 Ollama 或者 LMS 能直接使用的模型。这是一个败笔，这导致体系封闭，哪怕飞桨体系有好的技术创新或者模型创新，也很难在开源社区形成正向反馈。很可能百度又变成了AI时代的工程师摇篮，项目最终很难在从开源中收益，开发也就不会单独尝试一个私有ai体系。于是基本可以认为飞桨已死。
方案六，尝试了 Windows 体系之后，重新开始折腾 Linux，因为机器本身装的 Windows，切换的成本比较高，要重新装一个系统也不太现实，其它的 Linux 机器没有显卡可用。纠结了好久，最终选择尝试 Windows 下的 WSL。然后居然跑通了，11月底到现在，虽然不是每天都在弄这件事，依然是一个漫长的过程，一直在踩各种坑。

换了 WSL 之后，碰上的第一个问题是，按照 DeepSeekOCR 的官方文档，程序没有跑起来，问题的根源是官方推荐的cuda 版本11.8不支持50系显卡，50系显卡 Nvidia 做了架构升级，于是过去版本的 cuda 不再兼容。只能说 50系显卡还是太新了。

按装 50系显卡兼容的 cuda 版本
```
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
```

找对应环境的flash-attn https://github.com/Dao-AILab/flash-attention/releases，下载并且手动安装。

折腾的这段时间，看到一些有趣的项目，解决了技术依赖。
 - pdf-craft 这个基于 DeepSeekOCR 实现的PDF转换工具，可以完整将图片、数学公式一起转换了，并且保留排版信息。另外代码里面支持非常丰富的各国家语言，写的很用心。
 - doc-page-extractor  pdf-craft 的依赖，对 DeepSeekOCR 的直接封装。
 - anx-reader 开源阅读器，跨平台电子书阅读。
 - SurfSense 整个 AI 知识库工具，NotebookML 开源替代方案。
 - booklore 另一个开源电子书工具。
 - Poppler PDF 处理的瑞士军刀工具箱。
 
很多 AI 模型现在默认是 huggingface，这倒是我100G的VPN 流量在1月4号就消耗完了，反复下载。LMS、Ollama、vllm 对模型的管理方式完全不一样，真是在给网络添堵。国内的 modelscope 社区也很好，基本上就是走的完全兼容 huggingface 的方式，替换起来还是很方便的。ollama 和 lms 都可以直接运行 modelscope 上的 guff 模型，这一点减少大量尝试时间，对模型开发者非常友好。

12月各大模型厂商依然在持续发布新模型，DeepSeekOCR 之后，DeepSeek 又发了新的论文，改善模型训练时数据的时候方式。这两项优化对于 AI 的发生又是一次技术创新。阿里的 Qwen 在架构和训练结果上，已经拿到了相对好的开源社区结果。

从现在的情况看，2026 一定是 AI 应用密集发布的年份，因为大量基础模型的能力已经趋于稳定，密集的技术创新可能在未来几年不会像24年和25年这两年那么密集。大模型开始进入淘汰阶段，现在已经累计了优势的，可能未来几年能有一些领先优势。从技术的角度看，创新的不确定性可能不在模型层出现，而在硬件层。2026很可能出现新的数学运算方式，彻底改变现在的显卡模式。不过这个应该只会影响算力成本，而不会影响模型架构。

基本可以明确20026年 AI 应用会大爆发，AI相关的硬件创新也会不断出现。中国芯片制造能力的跟进，很可能立体化的影响 AI 的未来之路。